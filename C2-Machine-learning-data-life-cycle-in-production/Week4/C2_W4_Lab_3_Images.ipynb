{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab: Feature Engineering with Images\n",
    "\n",
    "In this optional notebook, you will be looking at how to prepare features with an image dataset, particularly [CIFAR-10](https://www.tensorflow.org/datasets/catalog/cifar10). You will mostly go through the same steps but you will need to add parser functions in your transform module to successfully read and convert the data. As with the previous notebooks, we will just go briefly over the early stages of the pipeline so you can focus on the Transform component.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-ePgV0Lj68Q"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIqpWK9efviJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.types import Channel\n",
    "\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufJKQ6OvkJlY"
   },
   "source": [
    "## Set up pipeline paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Se9m0HbDHBvv"
   },
   "outputs": [],
   "source": [
    "# Location of the pipeline metadata store\n",
    "_pipeline_root = './pipeline/'\n",
    "\n",
    "# Data files directory\n",
    "_data_root = './data/cifar10'\n",
    "\n",
    "# Path to the training data\n",
    "_data_filepath = os.path.join(_data_root, 'train.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2cMMAbSkGfX"
   },
   "source": [
    "## Download example data\n",
    "\n",
    "We will download the training split of the CIFAR-10 dataset and save it to the `_data_filepath`. Take note that this is already in TFRecord format so we won't need to convert it when we use `ExampleGen` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BywX6OUEhAqn"
   },
   "outputs": [],
   "source": [
    "# Create data folder for the images\n",
    "!mkdir -p {_data_root}\n",
    "\n",
    "# URL of the hosted dataset\n",
    "DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/v0.21.4/tfx/examples/cifar10/data/train.tfrecord'\n",
    "\n",
    "# Download the dataset and save locally\n",
    "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ONIE_hdkPS4"
   },
   "source": [
    "## Create the InteractiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Rh6K5sUf9dd"
   },
   "outputs": [],
   "source": [
    "# Initialize the InteractiveContext\n",
    "context = InteractiveContext(pipeline_root=_pipeline_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdQWxfsVkzdJ"
   },
   "source": [
    "## Run TFX components interactively\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9fwt9gQk3BR"
   },
   "source": [
    "### ExampleGen\n",
    "\n",
    "As mentioned earlier, the dataset is already in TFRecord format so, unlike the previous TFX labs, there is no need to convert it when we ingest the data. You can simply import it with [ImportExampleGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/ImportExampleGen) and here is the syntax and modules for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyXjuMt8f-9u"
   },
   "outputs": [],
   "source": [
    "# Ingest the data through ExampleGen\n",
    "example_gen = tfx.components.ImportExampleGen(input_base=_data_root)\n",
    "\n",
    "# Run the component\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqCoZh7KPUm9"
   },
   "source": [
    "As usual, this component produces two artifacts, training examples and evaluation examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "880KkTAkPeUg"
   },
   "outputs": [],
   "source": [
    "# Print split names and URI\n",
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "print(artifact.split_names, artifact.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6vcbW_wPqvl"
   },
   "source": [
    "You can also take a look at the first three training examples ingested by using the `tf.io.parse_single_example()` method from the [tf.io](https://www.tensorflow.org/api_docs/python/tf/io) module. See how it is setup in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4XIXjiCPwzQ"
   },
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "# Get the URI of the output artifact representing the training examples, which is a directory\n",
    "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Description per example\n",
    "image_feature_description = {\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "# Image parser function\n",
    "def _parse_image_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "# Map the parser to the dataset\n",
    "parsed_image_dataset = dataset.map(_parse_image_function)\n",
    "\n",
    "# Display the first three images\n",
    "for features in parsed_image_dataset.take(3):\n",
    "    image_raw = features['image_raw'].numpy()\n",
    "    display.display(display.Image(data=image_raw))\n",
    "    pprint.pprint('Class ID: {}'.format(features['label'].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csM6BFhtk5Aa"
   },
   "source": [
    "### StatisticsGen\n",
    "\n",
    "Next, you will generate the statistics so you can infer a schema in the next step. You can also look at the visualization of the statistics. As you might expect with CIFAR-10, there is a column for the image and another column for the numeric label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAscCCYWgA-9"
   },
   "outputs": [],
   "source": [
    "# Run StatisticsGen\n",
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLKLTO9Nk60p"
   },
   "source": [
    "### SchemaGen\n",
    "\n",
    "Here, you pass in the statistics to generate the Schema. For the version of TFX you are using, you will have to explicitly set `infer_feature_shape=True` so the downstream TFX components (e.g. Transform) will parse input as a `Tensor` and not `SparseTensor`. If not set, you will have compatibility issues later when you run the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygQvZ6hsiQ_J"
   },
   "outputs": [],
   "source": [
    "# Run SchemaGen\n",
    "schema_gen = tfx.components.SchemaGen(\n",
    "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ec9vqDXpXeMb"
   },
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1qcUuO9k9f8"
   },
   "source": [
    "### ExampleValidator\n",
    "\n",
    "`ExampleValidator` is not required but you can still run it just to make sure that there are no anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRlRUuGgiXks"
   },
   "outputs": [],
   "source": [
    "# Run ExampleValidator\n",
    "example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDyAAozQcrk3"
   },
   "outputs": [],
   "source": [
    "# Visualize the results. There should be no anomalies.\n",
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPViEz5RlA36"
   },
   "source": [
    "### Transform\n",
    "\n",
    "To successfully transform the raw image, you need to parse the current bytes format and convert it to a tensor. For that, you can use the [tf.image.decode_image()](https://www.tensorflow.org/api_docs/python/tf/io/decode_image) function. The transform module below utilizes this and converts the image to a `(32,32,3)` shaped float tensor. It also scales the pixels and converts the labels to one-hot tensors. The output features should then be ready to pass on to a model that accepts this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AJ9hBs94YJm"
   },
   "outputs": [],
   "source": [
    "_transform_module_file = 'cifar10_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYmxxx9A4YJn"
   },
   "outputs": [],
   "source": [
    "%%writefile {_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Keys\n",
    "_LABEL_KEY = 'label'\n",
    "_IMAGE_KEY = 'image_raw'\n",
    "\n",
    "\n",
    "def _transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "def _image_parser(image_str):\n",
    "    '''converts the images to a float tensor'''\n",
    "    image = tf.image.decode_image(image_str, channels=3)\n",
    "    image = tf.reshape(image, (32, 32, 3))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "\n",
    "def _label_parser(label_id):\n",
    "    '''one hot encodes the labels'''\n",
    "    label = tf.one_hot(label_id, 10)\n",
    "    return label\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "        inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "        Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the raw image and labels to a float array and\n",
    "    # one-hot encoded labels, respectively.\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        outputs = {\n",
    "            _transformed_name(_IMAGE_KEY):\n",
    "                tf.map_fn(\n",
    "                    _image_parser,\n",
    "                    tf.squeeze(inputs[_IMAGE_KEY], axis=1),\n",
    "                    dtype=tf.float32),\n",
    "            _transformed_name(_LABEL_KEY):\n",
    "                tf.map_fn(\n",
    "                    _label_parser,\n",
    "                    tf.squeeze(inputs[_LABEL_KEY], axis=1),\n",
    "                    dtype=tf.float32)\n",
    "        }\n",
    "    \n",
    "    # scale the pixels from 0 to 1\n",
    "    outputs[_transformed_name(_IMAGE_KEY)] = tft.scale_to_0_1(outputs[_transformed_name(_IMAGE_KEY)])\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgbmZr3sgbWW"
   },
   "source": [
    "Now, we pass in this feature engineering code to the `Transform` component and run it to transform your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHfhth_GiZI9"
   },
   "outputs": [],
   "source": [
    "# Ignore TF warning messages\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Setup the Transform component\n",
    "transform = tfx.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(_transform_module_file))\n",
    "\n",
    "# Run the component\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the results\n",
    "\n",
    "Now that the Transform component is finished, you can preview how the transformed images and labels look like. You can use the same sequence and helper function from previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwbW2zPKR_S4"
   },
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to get individual examples\n",
    "def get_records(dataset, num_records):\n",
    "    '''Extracts records from the given dataset.\n",
    "    Args:\n",
    "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
    "        num_records (int): number of records to preview\n",
    "    '''\n",
    "    \n",
    "    # initialize an empty list\n",
    "    records = []\n",
    "    \n",
    "    # Use the `take()` method to specify how many records to get\n",
    "    for tfrecord in dataset.take(num_records):\n",
    "        \n",
    "        # Get the numpy property of the tensor\n",
    "        serialized_example = tfrecord.numpy()\n",
    "        \n",
    "        # Initialize a `tf.train.Example()` to read the serialized data\n",
    "        example = tf.train.Example()\n",
    "        \n",
    "        # Read the example data (output is a protocol buffer message)\n",
    "        example.ParseFromString(serialized_example)\n",
    "        \n",
    "        # convert the protocol bufffer message to a Python dictionary\n",
    "        example_dict = (MessageToDict(example))\n",
    "        \n",
    "        # append to the records list\n",
    "        records.append(example_dict)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see from the output of the cell below that the transformed raw image (i.e. `image_raw_xf`) now has a float array that is scaled from 0 to 1. Similarly, you'll see that the transformed label (i.e. `label_xf`) is now one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSDZ2rJC7NQW"
   },
   "outputs": [],
   "source": [
    "# Get 1 record from the dataset\n",
    "sample_records = get_records(dataset, 1)\n",
    "\n",
    "# Print the output\n",
    "pp.pprint(sample_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Up\n",
    "\n",
    "This notebook demonstrates how to do feature engineering with image datasets as opposed to simple tabular data. This should come in handy in your computer vision projects and you can also try replicating this process with other image datasets from [TFDS](https://www.tensorflow.org/datasets/catalog/overview#image_classification)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "L3_Images_TFX_Transform_Component.ipynb",
   "provenance": [
    {
     "file_id": "15Dip60LKwQwGZ56z_DpI0inWgTc57T1X",
     "timestamp": 1601093693126
    },
    {
     "file_id": "1zwpIqHj_QW6dszaqdgviqL6yrxtk9WOJ",
     "timestamp": 1600118027706
    },
    {
     "file_id": "1hG1J8RRAPtsJZFCsYHWatuJx57dJ66Jn",
     "timestamp": 1589189499813
    },
    {
     "file_id": "https://github.com/tensorflow/tfx/blob/master/docs/tutorials/tfx/components_keras.ipynb",
     "timestamp": 1588766915739
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
